{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ee6126-f815-4456-85ca-459177f91cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "models/Llama-3-8B-Instruct-Gradient-1048k is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/models/Llama-3-8B-Instruct-Gradient-1048k/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/file_download.py:1339\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/file_download.py:1746\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1746\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/file_download.py:1666\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1666\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1675\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/file_download.py:364\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 364\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/file_download.py:388\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    387\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 388\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67e377ed-5d2a602c1f42b9e50b4dc5c5;d50103bc-f14d-4167-87f3-f9a9fc81b7e0)\n\nRepository Not Found for url: https://huggingface.co/models/Llama-3-8B-Instruct-Gradient-1048k/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/Llama-3-8B-Instruct-Gradient-1048k\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     ckpt,\n\u001b[1;32m     13\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     14\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(ckpt)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:844\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 844\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    846\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:676\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    675\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 676\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: models/Llama-3-8B-Instruct-Gradient-1048k is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# Load the model\n",
    "ckpt = \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(ckpt)\n",
    "eos_token_ids = generation_config.eos_token_id\n",
    "if not isinstance(eos_token_ids, list):\n",
    "    eos_token_ids = [eos_token_ids]\n",
    "\n",
    "# add some tokens like \"</user>\" and </s> to eos ids\n",
    "eos_token_ids += tokenizer.encode(\"</user>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</s>\", add_special_tokens=False)\n",
    "eos_token_ids += tokenizer.encode(\"</\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dd6ce-0997-4574-99a1-085cfe7c1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duo_attn.utils import load_attn_pattern, sparsify_attention_heads\n",
    "from duo_attn.patch import enable_duo_attention_eval\n",
    "\n",
    "# Load the attention pattern\n",
    "attn_heads, sink_size, recent_size = load_attn_pattern(\n",
    "    \"attn_patterns/Llama-3-8B-Instruct-Gradient-1048k/lr=0.02-reg=0.05-ctx=1000_32000-multi_passkey10\"\n",
    ")\n",
    "\n",
    "print(attn_heads.shape)\n",
    "print(sink_size)\n",
    "print(recent_size)\n",
    "\n",
    "# Sparsify attention heads\n",
    "attn_heads, sparsity = sparsify_attention_heads(attn_heads, sparsity=0.5)\n",
    "\n",
    "print(attn_heads, sparsity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a543c4-370e-4d7e-9045-ce2eb17885c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import (\n",
    "    logger,\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv,\n",
    "    LlamaSdpaAttention,\n",
    "    LlamaFlashAttention2,\n",
    ")\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache, OffloadedCache, OffloadedStaticCache\n",
    "import types\n",
    "from transformers.modeling_flash_attention_utils  import _flash_attention_forward\n",
    "\n",
    "# def LlamaAttention_fast_forward(\n",
    "#     self,\n",
    "#     hidden_states: torch.Tensor,\n",
    "#     attention_mask: Optional[torch.Tensor] = None,\n",
    "#     position_ids: Optional[torch.LongTensor] = None,\n",
    "#     past_key_value: Optional[Cache] = None,\n",
    "#     output_attentions: bool = False,\n",
    "#     use_cache: bool = False,\n",
    "#     cache_position: Optional[torch.LongTensor] = None,\n",
    "#     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "#     **kwargs,\n",
    "# ) :\n",
    "#     # LlamaFlashAttention2 attention does not support output_attentions\n",
    "#     output_attentions = False\n",
    "\n",
    "#     bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "#     query_states = self.q_proj(hidden_states)\n",
    "#     key_states = self.k_proj(hidden_states)\n",
    "#     value_states = self.v_proj(hidden_states)\n",
    "\n",
    "#     # Flash attention requires the input to have the shape\n",
    "#     # batch_size x seq_length x head_dim x hidden_dim\n",
    "#     # therefore we just need to keep the original shape\n",
    "#     query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#     key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "#     value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "#     cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "    \n",
    "#     query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "#     if past_key_value is not None:\n",
    "#         # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "#         cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "#         key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "#     # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n",
    "#     # to be able to avoid many of these transpose/reshape/view.\n",
    "#     query_states = query_states.transpose(1, 2)\n",
    "#     key_states = key_states.transpose(1, 2)\n",
    "#     value_states = value_states.transpose(1, 2)\n",
    "\n",
    "#     dropout_rate = self.attention_dropout if self.training else 0.0\n",
    "\n",
    "#     # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n",
    "#     # therefore the input hidden states gets silently casted in float32. Hence, we need\n",
    "#     # cast them back in the correct dtype just to be sure everything works as expected.\n",
    "#     # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n",
    "#     # in fp32. (LlamaRMSNorm handles it correctly)\n",
    "#     input_dtype = query_states.dtype\n",
    "#     if input_dtype == torch.float32:\n",
    "#         query_states = query_states.to(target_dtype)\n",
    "#         key_states = key_states.to(target_dtype)\n",
    "#         value_states = value_states.to(target_dtype)\n",
    "\n",
    "#     attn_output = _flash_attention_forward(\n",
    "#         query_states,\n",
    "#         key_states,\n",
    "#         value_states,\n",
    "#         attention_mask,\n",
    "#         q_len,\n",
    "#         position_ids=position_ids,\n",
    "#         dropout=dropout_rate,\n",
    "#         sliding_window=getattr(self, \"sliding_window\", None),\n",
    "#         use_top_left_mask=self._flash_attn_uses_top_left_mask,\n",
    "#         is_causal=self.is_causal,\n",
    "#     )\n",
    "\n",
    "#     attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n",
    "#     attn_output = self.o_proj(attn_output)\n",
    "\n",
    "#     if not output_attentions:\n",
    "#         attn_weights = None\n",
    "\n",
    "#     return attn_output, attn_weights, past_key_value\n",
    "\n",
    "# def enable_duo_attention_eval(model, full_attention_heads):\n",
    "    \n",
    "#     device = next(model.parameters()).device\n",
    "#     dtype = next(model.parameters()).dtype\n",
    "#     for idx, layer in enumerate(model.model.layers):\n",
    "#         module = layer.self_attn\n",
    "#         layer_full_attention_heads = torch.tensor(\n",
    "#             full_attention_heads[idx], device=device, dtype=dtype\n",
    "#         )\n",
    "#         module.full_attn_head_mask = layer_full_attention_heads > 0.5\n",
    "#         module.num_full_attn_head = module.full_attn_head_mask.sum().item()\n",
    "#         module.num_streaming_attn_head = (module.num_key_value_heads - module.num_full_attn_head)\n",
    "#         print(layer_full_attention_heads)\n",
    "#         module.forward = types.MethodType(\n",
    "#             LlamaAttention_fast_forward, module\n",
    "#         )\n",
    "\n",
    "# enable_duo_attention_eval(model, attn_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cee988-d081-4174-8f1f-f0f2f117a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LlamaAttention_fast_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") :\n",
    "    output_attentions = False\n",
    "\n",
    "    bsz, q_len, hd = hidden_states.size()\n",
    "    chunk_size = hd // self.num_key_value_heads\n",
    "    num_heads = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "    if not hasattr(self, 'q_proj_list'):\n",
    "        self.q_proj_list = list((self.q_proj.weight.split(self.head_dim * num_heads, dim=0)))\n",
    "        # self.q_proj.weight.data.storage().resize_(0)\n",
    "    if not hasattr(self, 'k_proj_list'):\n",
    "        self.k_proj_list = list((self.k_proj.weight.split(self.head_dim, dim=0)))\n",
    "        # self.k_proj.weight.data.storage().resize_(0)\n",
    "    if not hasattr(self, 'v_proj_list'):\n",
    "        self.v_proj_list = list((self.v_proj.weight.split(self.head_dim, dim=0)))\n",
    "        # self.v_proj.weight.data.storage().resize_(0)\n",
    "\n",
    "\n",
    "    attn_output_list = [None for _ in range((self.num_key_value_heads))]\n",
    "    \n",
    "    for i in range(self.num_key_value_heads):\n",
    "        bsz, q_len, hd = hidden_states.size()\n",
    "\n",
    "        self.q_proj.weight.data = self.q_proj_list[i].data\n",
    "        self.k_proj.weight.data = self.k_proj_list[i].data\n",
    "        self.v_proj.weight.data = self.v_proj_list[i].data\n",
    "\n",
    "        # print(hidden_states.shape, self.q_proj.weight.shape)\n",
    "        \n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Flash attention requires the input to have the shape\n",
    "        # batch_size x seq_length x head_dim x hidden_dim\n",
    "        # therefore we just need to keep the original shape\n",
    "        query_states = query_states.view(bsz, q_len, num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "    \n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position, 'full_head': self.full_attn_head_mask[i]}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx + i, cache_kwargs)\n",
    "    \n",
    "        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n",
    "        # to be able to avoid many of these transpose/reshape/view.\n",
    "        query_states = query_states.transpose(1, 2)\n",
    "        key_states = key_states.transpose(1, 2)\n",
    "        value_states = value_states.transpose(1, 2)\n",
    "    \n",
    "        dropout_rate = self.attention_dropout if self.training else 0.0\n",
    "    \n",
    "        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n",
    "        # therefore the input hidden states gets silently casted in float32. Hence, we need\n",
    "        # cast them back in the correct dtype just to be sure everything works as expected.\n",
    "        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n",
    "        # in fp32. (LlamaRMSNorm handles it correctly)\n",
    "    \n",
    "        input_dtype = query_states.dtype\n",
    "        if input_dtype == torch.float32:\n",
    "            if torch.is_autocast_enabled():\n",
    "                target_dtype = torch.get_autocast_gpu_dtype()\n",
    "            # Handle the case where the model is quantized\n",
    "            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n",
    "                target_dtype = self.config._pre_quantization_dtype\n",
    "            else:\n",
    "                target_dtype = self.q_proj.weight.dtype\n",
    "    \n",
    "            logger.warning_once(\n",
    "                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n",
    "                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n",
    "                f\" {target_dtype}.\"\n",
    "            )\n",
    "    \n",
    "            query_states = query_states.to(target_dtype)\n",
    "            key_states = key_states.to(target_dtype)\n",
    "            value_states = value_states.to(target_dtype)\n",
    "    \n",
    "        attn_output = _flash_attention_forward(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            q_len,\n",
    "            position_ids=position_ids,\n",
    "            dropout=dropout_rate,\n",
    "            sliding_window=getattr(self, \"sliding_window\", None),\n",
    "            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n",
    "            is_causal=self.is_causal,\n",
    "        )\n",
    "    \n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n",
    "        attn_output_list[i] = attn_output\n",
    "        \n",
    "    attn_output = torch.cat(attn_output_list, dim=-1)\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    if not output_attentions:\n",
    "        attn_weights = None\n",
    "\n",
    "    return attn_output, attn_weights, past_key_value\n",
    "\n",
    "full_attention_heads = attn_heads\n",
    "LlamaFlashAttention2.forward = LlamaAttention_fast_forward\n",
    "layer_idx = 0\n",
    "for idx, layer in enumerate(model.model.layers):\n",
    "    device = next(model.parameters()).device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "    module = layer.self_attn\n",
    "    module.layer_idx = layer_idx\n",
    "    layer_idx += module.num_key_value_heads\n",
    "\n",
    "    # print(full_attention_heads)\n",
    "    # layer_full_attention_heads = torch.tensor(\n",
    "    #     full_attention_heads[idx], device=device, dtype=dtype\n",
    "    # )\n",
    "    module.full_attn_head_mask = full_attention_heads[idx] > 0.5\n",
    "    module.num_full_attn_head = module.full_attn_head_mask.sum().item()\n",
    "    module.num_streaming_attn_head = (module.num_key_value_heads - module.num_full_attn_head)\n",
    "    print(module.full_attn_head_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca07e3-d1e7-435f-bf5b-dbded026079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OffloadedCache(DynamicCache):\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "\n",
    "        >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Prepare a cache class and pass it to model's forward\n",
    "        >>> past_key_values = DynamicCache()\n",
    "        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n",
    "        >>> outputs.past_key_values # access cache filled with key/values from generation\n",
    "        DynamicCache()\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"OffloadedCache can only be used with a GPU\")\n",
    "        super().__init__()\n",
    "        self.original_device = []\n",
    "        self.prefetch_stream = torch.cuda.Stream()\n",
    "        self.beam_idx = None  # used to delay beam search operations\n",
    "\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        \n",
    "        self.offload_key_cache: List[torch.Tensor] = []\n",
    "        self.offload_value_cache: List[torch.Tensor] = []\n",
    "\n",
    "        self.id_type_list = []\n",
    "        self.real_id_dict = {}\n",
    "\n",
    "    def prefetch_layer(self, layer_idx: int):\n",
    "        \"Starts prefetching the next layer cache\"\n",
    "        if layer_idx < len(self.offload_key_cache):\n",
    "            with torch.cuda.stream(self.prefetch_stream):\n",
    "                # Prefetch next layer tensors to GPU\n",
    "                device = self.original_device[layer_idx]\n",
    "                self.offload_key_cache[layer_idx] = self.offload_key_cache[layer_idx].to(device, non_blocking=True)\n",
    "                self.offload_value_cache[layer_idx] = self.offload_value_cache[layer_idx].to(device, non_blocking=True)\n",
    "    \n",
    "    def evict_previous_layer(self, layer_idx: int):\n",
    "        \"Moves the previous layer cache to the CPU\"\n",
    "        if len(self.offload_key_cache) > 2:\n",
    "            # We do it on the default stream so it occurs after all earlier computations on these tensors are done\n",
    "            prev_layer_idx = (layer_idx - 1) % len(self.offload_key_cache)\n",
    "            self.offload_key_cache[prev_layer_idx] = self.offload_key_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "            self.offload_value_cache[prev_layer_idx] = self.offload_value_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n",
    "            \n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n",
    "        sequence length.\n",
    "        \"\"\"\n",
    "        if layer_idx < len(self.id_type_list):\n",
    "            if self.id_type_list[layer_idx] == True:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "                return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "            else:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "                # Evict the previous layer if necessary\n",
    "                torch.cuda.current_stream().synchronize()\n",
    "                self.evict_previous_layer(layer_idx)\n",
    "                # Load current layer cache to its original device if not already there\n",
    "                original_device = self.original_device[layer_idx]\n",
    "                self.prefetch_stream.synchronize()\n",
    "                key_tensor = self.offload_key_cache[layer_idx]\n",
    "                value_tensor = self.offload_value_cache[layer_idx]\n",
    "                # Now deal with beam search ops which were delayed\n",
    "                if self.beam_idx is not None:\n",
    "                    self.beam_idx = self.beam_idx.to(original_device)\n",
    "                    key_tensor = key_tensor.index_select(0, self.beam_idx)\n",
    "                    value_tensor = value_tensor.index_select(0, self.beam_idx)\n",
    "                # Prefetch the next layer\n",
    "                self.prefetch_layer((layer_idx + 1) % len(self))\n",
    "                return (key_tensor, value_tensor)\n",
    "        else:\n",
    "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "\n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
    "        \"\"\"Saves the beam indices and reorders the cache when the tensor is back to its device.\"\"\"\n",
    "        # We delay this operation until the tensors are back to their original\n",
    "        # device because performing torch.index_select on the CPU is very slow\n",
    "        del self.beam_idx\n",
    "        self.beam_idx = beam_idx.clone()\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
    "\n",
    "        Parameters:\n",
    "            key_states (`torch.Tensor`):\n",
    "                The new key states to cache.\n",
    "            value_states (`torch.Tensor`):\n",
    "                The new value states to cache.\n",
    "            layer_idx (`int`):\n",
    "                The index of the layer to cache the states for.\n",
    "            cache_kwargs (`Dict[str, Any]`, `optional`):\n",
    "                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
    "\n",
    "        Return:\n",
    "            A tuple containing the updated key and value states.\n",
    "        \"\"\"\n",
    "        # Update the number of seen tokens\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        is_stream_head = (cache_kwargs['full_head'] == False)\n",
    "        if len(self.id_type_list) <= layer_idx:\n",
    "            self.id_type_list.append(is_stream_head)\n",
    "\n",
    "        original_layer_layer_idx = layer_idx\n",
    "\n",
    "        # print(layer_idx, cache_kwargs['full_head'])\n",
    "\n",
    "        if is_stream_head:\n",
    "            if layer_idx in self.real_id_dict:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "            else:\n",
    "                self.real_id_dict[layer_idx] = len(self.key_cache)\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "            if len(self.key_cache) <= layer_idx:\n",
    "                # There may be skipped layers, fill them with empty lists\n",
    "                for _ in range(len(self.key_cache), layer_idx):\n",
    "                    self.key_cache.append([])\n",
    "                    self.value_cache.append([])\n",
    "                self.key_cache.append(key_states)\n",
    "                self.value_cache.append(value_states)\n",
    "            elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
    "                self.key_cache[layer_idx] = key_states\n",
    "                self.value_cache[layer_idx] = value_states\n",
    "            else:\n",
    "                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "                \n",
    "            key_tensor, value_tensor = self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "            incoming_kv_seq_len = self.key_cache[layer_idx].shape[2]\n",
    "            if incoming_kv_seq_len > self.sink_size + self.recent_size:\n",
    "                sink_key_states = self.key_cache[layer_idx][:, :, : self.sink_size, :]\n",
    "                recent_key_states = self.key_cache[layer_idx][\n",
    "                    :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "                ]\n",
    "                self.key_cache[layer_idx] = torch.cat([sink_key_states, recent_key_states], dim=-2)\n",
    "\n",
    "                sink_value_states = self.value_cache[layer_idx][:, :, : self.sink_size, :]\n",
    "                recent_value_states = self.value_cache[layer_idx][\n",
    "                    :, :, incoming_kv_seq_len - self.recent_size : incoming_kv_seq_len, :\n",
    "                ]\n",
    "                self.value_cache[layer_idx] = torch.cat([sink_value_states, recent_value_states], dim=-2)\n",
    "        else:\n",
    "            if layer_idx in self.real_id_dict:\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "            else:\n",
    "                self.real_id_dict[layer_idx] = len(self.offload_key_cache)\n",
    "                layer_idx = self.real_id_dict[layer_idx]\n",
    "                \n",
    "            if len(self.offload_key_cache) < layer_idx:\n",
    "                raise ValueError(\"OffloadedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n",
    "            elif len(self.offload_key_cache) == layer_idx:\n",
    "                self.offload_key_cache.append(key_states)\n",
    "                self.offload_value_cache.append(value_states)\n",
    "                self.original_device.append(key_states.device)\n",
    "                self.evict_previous_layer(layer_idx)\n",
    "                key_tensor, value_tensor = key_states, value_states\n",
    "            else:\n",
    "                key_tensor, value_tensor = self[original_layer_layer_idx]\n",
    "                self.offload_key_cache[layer_idx] = torch.cat([key_tensor, key_states], dim=-2)\n",
    "                self.offload_value_cache[layer_idx] = torch.cat([value_tensor, value_states], dim=-2)\n",
    "                key_tensor = self.offload_key_cache[layer_idx]\n",
    "                value_tensor = self.offload_value_cache[layer_idx]\n",
    "\n",
    "        return key_tensor, value_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6582c1-ade6-452b-b6f6-2ccd5d29605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize past_key_values to None\n",
    "past_key_values = OffloadedCache()\n",
    "past_key_values.sink_size = 64\n",
    "past_key_values.recent_size = 256\n",
    "# Manually perform inference using KV cache\n",
    "\n",
    "inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n",
    "max_new_tokens = 23\n",
    "generated_tokens = []\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "        \n",
    "        # Extract the logits and past_key_values (the cache)\n",
    "        next_token_logits = outputs.logits[:, -1, :]  # Logits of the last token\n",
    "        past_key_values = outputs.past_key_values  # KV cache to be reused in the next step\n",
    "\n",
    "        # Greedy decoding: get the token with the highest probability\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "        generated_tokens.append(next_token.item())\n",
    "\n",
    "        # Only pass the new token for the next iteration\n",
    "        input_ids = next_token.unsqueeze(-1)\n",
    "\n",
    "# Convert generated token ids to text\n",
    "output_text = \"Fun fact: The shortest\" + tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c426515-05da-45e4-b748-2ba4444224ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
