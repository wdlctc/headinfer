{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8658036a70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from transformers.generation import GenerationConfig\n",
    "from hip_attn.models.modeling_llama import LlamaForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c270adafb61e403da0bc5e71704dc3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache, OffloadedCache, OffloadedStaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  0.33 seconds\n",
      "Epoch 2  0.02 seconds\n",
      "Epoch 3  0.02 seconds\n",
      "Epoch 4  0.02 seconds\n",
      "Epoch 5  0.02 seconds\n",
      "Epoch 6  0.02 seconds\n",
      "Epoch 7  0.02 seconds\n",
      "Epoch 8  0.02 seconds\n",
      "Epoch 9  0.02 seconds\n",
      "Epoch 10  0.02 seconds\n",
      "Epoch 1  0.02 seconds\n",
      "Epoch 2  0.02 seconds\n",
      "Epoch 3  0.02 seconds\n",
      "Epoch 4  0.02 seconds\n",
      "Epoch 5  0.02 seconds\n",
      "Epoch 6  0.02 seconds\n",
      "Epoch 7  0.02 seconds\n",
      "Epoch 8  0.02 seconds\n",
      "Epoch 9  0.02 seconds\n",
      "Epoch 10  0.02 seconds\n",
      "Epoch 1  0.02 seconds\n",
      "Epoch 2  0.02 seconds\n",
      "Epoch 3  0.02 seconds\n",
      "Epoch 4  0.02 seconds\n",
      "Epoch 5  0.02 seconds\n",
      "Epoch 6  0.02 seconds\n",
      "Epoch 7  0.02 seconds\n",
      "Epoch 8  0.02 seconds\n",
      "Epoch 9  0.02 seconds\n",
      "Epoch 10  0.02 seconds\n",
      "Epoch 1  0.02 seconds\n",
      "Epoch 2  0.02 seconds\n",
      "Epoch 3  0.02 seconds\n",
      "Epoch 4  0.02 seconds\n",
      "Epoch 5  0.02 seconds\n",
      "Epoch 6  0.02 seconds\n",
      "Epoch 7  0.02 seconds\n",
      "Epoch 8  0.02 seconds\n",
      "Epoch 9  0.02 seconds\n",
      "Epoch 10  0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.inference_mode():\n",
    "    ### Simulate Prefill\n",
    "    start_length = 10240\n",
    "    chunks = 4\n",
    "    input_ids = torch.randint(0, tokenizer.vocab_size, (1, start_length)).to('cuda')\n",
    "    # for epoch in range(4):\n",
    "    #     past_key_values = OffloadedCache()\n",
    "\n",
    "    #     for i in range(chunks):\n",
    "            \n",
    "    #         torch.cuda.synchronize()\n",
    "    #         start_time = time.time()\n",
    "\n",
    "    #         model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "\n",
    "    #         torch.cuda.synchronize()\n",
    "    #         epoch_time = time.time() - start_time\n",
    "    #         print(f\"[Epoch {epoch+1}, Chunk {i+1}]   {epoch_time:.2f} seconds\")\n",
    "\n",
    "    #     del past_key_values\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     torch.cuda.synchronize()\n",
    "\n",
    "    past_key_values = DynamicCache()\n",
    "    config = model.config\n",
    "\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        device = next(model.parameters()).device\n",
    "        dtype = next(model.parameters()).dtype\n",
    "        module = layer.self_attn\n",
    "\n",
    "        # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n",
    "        head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n",
    "        # head_dim = config.hidden_size\n",
    "        cache_shape = (1, config.num_key_value_heads, start_length, module.head_dim)\n",
    "        key_states = torch.zeros(cache_shape, dtype=dtype, device=device)\n",
    "        value_states = torch.zeros(cache_shape, dtype=dtype, device=device)\n",
    "\n",
    "        key_states, value_states = past_key_values.update(key_states, value_states, module.layer_idx)\n",
    "\n",
    "\n",
    "    ### Simulate Decode\n",
    "    start_length = 10240\n",
    "\n",
    "    max_new_tokens = 10\n",
    "    input_ids = torch.randint(0, tokenizer.vocab_size, (1, 1)).to('cuda')\n",
    "\n",
    "    for epoch in range(4):\n",
    "        for i in range(max_new_tokens):\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Epoch {i+1}  {epoch_time:.2f} seconds\")\n",
    "        start_length = start_length + 5120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# context = \"A quick brown fox jumps over the lazy dog. \\n\"\n",
    "# # with open(\"demo/duo_attention.txt\", \"r\") as f:\n",
    "# #     needle = f.read()\n",
    "# needle=\"\\n\\nRemember, the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\\n\"\n",
    "# num_tokens_context = len(tokenizer.encode(context, add_special_tokens=False))\n",
    "# num_repetitions = 1000000 // num_tokens_context\n",
    "\n",
    "# text = (\n",
    "#     \"This is a very long story book: <book> \"\n",
    "#     + context * int(num_repetitions * 0.75)\n",
    "#     + needle\n",
    "#     + context * int(num_repetitions * (1 - 0.75))\n",
    "#     + \"what is the best thing to do in San Francisco?\\n\\nAnswer: The best thing to do in San Francisco is\"\n",
    "# )\n",
    "\n",
    "# input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model.generate(input_ids, do_sample=False, max_new_tokens=1)\n",
    "# output = tokenizer.decode(output.cpu()[0], skip_special_tokens=False)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
