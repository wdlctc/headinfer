{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77616457db50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from transformers.generation import GenerationConfig\n",
    "from hip_attn.models.modeling_llama import LlamaForCausalLM\n",
    "# from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868fa2c6f7e94797ad63911112a9ea0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache, OffloadedCache, OffloadedStaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Chunk 1]   1.21 seconds\n",
      "[Epoch 2, Chunk 1]   0.12 seconds\n",
      "[Epoch 3, Chunk 1]   0.12 seconds\n",
      "[Epoch 4, Chunk 1]   0.12 seconds\n",
      "Epoch 100  0.04 seconds\n",
      "Epoch 100  0.04 seconds\n",
      "Epoch 100  0.03 seconds\n",
      "Epoch 100  0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.inference_mode():\n",
    "    ### Simulate Prefill\n",
    "    start_length = 1024\n",
    "    chunks = 1\n",
    "    input_ids = torch.randint(0, tokenizer.vocab_size, (1, start_length)).to('cuda')\n",
    "    for epoch in range(4):\n",
    "        past_key_values = DynamicCache()\n",
    "\n",
    "        for i in range(chunks):\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "\n",
    "            model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"[Epoch {epoch+1}, Chunk {i+1}]   {epoch_time:.2f} seconds\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    ### Simulate Decode\n",
    "    start_length = 10240\n",
    "\n",
    "    max_new_tokens = 100\n",
    "    input_ids = torch.randint(0, tokenizer.vocab_size, (1, 1)).to('cuda')\n",
    "\n",
    "    for epoch in range(4):\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        for i in range(max_new_tokens):\n",
    "            \n",
    "            model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True, num_logits_to_keep=1)\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        epoch_time = (time.time() - start_time)/max_new_tokens\n",
    "        print(f\"Epoch {i+1}  {epoch_time:.2f} seconds\")\n",
    "        start_length = start_length + 5120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaCustomAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (hyper_attention): HyperAttention(\n",
      "            (lsh): AngularLSH(num_proj=7, proj_dir.shape=torch.Size([1, 1, 128, 7]))\n",
      "          )\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
